<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The SLM Optimization Challenge</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007AFF;
            --secondary-color: #5856D6;
            --bg-color: #F2F2F7;
            --container-bg: #FFFFFF;
            --text-color: #1D1D1F;
            --subtle-text-color: #6E6E73;
            --border-color: #E5E5EA;
            --success-color: #34C759;
            --warning-color: #FF9500;
        }

        *, *::before, *::after {
            box-sizing: border-box;
        }

        body {
            font-family: 'Lato', 'Roboto', -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            line-height: 1.65;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 1rem;
        }

        .container {
            max-width: 800px;
            margin: 1rem auto;
            background-color: var(--container-bg);
            padding: 1.5rem 2rem;
            border-radius: 16px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.07);
        }

        h1, h2, h3, h4 {
            font-family: 'Roboto', sans-serif;
            color: #000;
            line-height: 1.3;
        }

        h1 {
            text-align: center;
            font-size: clamp(2em, 5vw, 2.8em);
            font-weight: 700;
            margin-bottom: 1.5rem;
        }
        
        h2 {
            font-size: clamp(1.5em, 4vw, 2em);
            border-bottom: 2px solid var(--border-color);
            padding-bottom: 0.75rem;
            margin-top: 2.5rem;
        }
        
        h3 {
             font-size: clamp(1.2em, 3.5vw, 1.5em);
        }

        p, li {
            font-size: 1.05em;
            color: var(--subtle-text-color);
        }

        .mission-brief {
            background-color: #F0F8FF;
            border-left: 5px solid var(--primary-color);
            padding: 1.5rem;
            margin: 2rem 0;
            border-radius: 8px;
        }
        .mission-brief p {
            margin: 0;
            color: #2c3e50;
        }

        details {
            margin-bottom: 1rem;
            border: 1px solid var(--border-color);
            border-radius: 12px;
            overflow: hidden;
            transition: box-shadow 0.3s ease;
        }
        details:hover {
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
        }

        summary {
            font-weight: 700;
            font-size: 1.2em;
            padding: 1rem 1.5rem;
            background-color: #FAFAFC;
            cursor: pointer;
            outline: none;
            display: flex;
            align-items: center;
            justify-content: space-between;
        }
        
        summary::after {
            content: '‚ñº';
            font-size: 0.8em;
            transform: rotate(-90deg);
            transition: transform 0.2s ease-in-out;
        }

        details[open] > summary::after {
            transform: rotate(0deg);
        }

        .step-content {
            padding: 1.5rem;
            border-top: 1px solid var(--border-color);
            background-color: var(--container-bg);
        }
        
        .step-content strong {
            color: var(--text-color);
        }

        .results {
            background-color: #F6FFF8;
            padding: 1.5rem;
            margin-top: 1.5rem;
            border: 1px solid #D6F5DD;
            border-radius: 8px;
        }
        .results h4 {
            color: var(--success-color);
            margin-top: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 1.5rem;
            font-size: 0.95em;
        }
        th, td {
            border: 1px solid var(--border-color);
            padding: 0.75rem;
            text-align: left;
        }
        th {
            background-color: #f7f7f7;
            font-weight: 700;
        }
        tr:nth-child(even) {
            background-color: #fcfcfc;
        }
        .gain {
            color: var(--success-color);
            font-weight: bold;
        }

        .final-conclusion {
            background-color: #FFF9E6;
            border-left: 5px solid var(--warning-color);
            padding: 1.5rem;
            margin-top: 3rem;
            border-radius: 8px;
        }
        .final-conclusion h3 {
            color: #BF7100;
            margin-top: 0;
        }
    </style>
</head>
<body>

    <div class="container">
        <h1>üéÆ SLM Optimization Challenge</h1>
        <div class="mission-brief">
            <p><strong>Your Mission:</strong> As an AI researcher, your goal is to enhance a Small Language Model (SLM). You must improve its performance without increasing its size, making it powerful yet efficient for devices like phones. This challenge is based on the paper: "A Post-Training Enhanced Optimization Approach for Small Language Models." Follow the steps to achieve a breakthrough! üöÄ</p>
        </div>

        <h2>Phase 1: Building a High-Quality Dataset üèóÔ∏è</h2>
        <p>Your first task is to construct a top-tier "alignment dataset." The paper proposes a method using guidance from large models to optimize data diversity and accuracy.</p>

        <details>
            <summary>Step 1: Collect & Standardize Data</summary>
            <div class="step-content">
                <p>You start by gathering data from various open-source datasets on the internet, covering topics like math, coding, and general instructions. You collect a massive 5 million records! Datasets include BelleGroup, Coig, Camel, Dolly, and Alpaca_GPT4. You then standardize the format of all this data and perform a preliminary classification.</p>
            </div>
        </details>

        <details>
            <summary>Step 2: Deduplication & Diversity</summary>
            <div class="step-content">
                <p>To avoid repetition, you use the <strong>simhash algorithm</strong> to find and remove duplicate prompts. To ensure data diversity, you use a larger model (Qwen2.5-7B-Instruct) to assign a secondary classification label to each instruction. Using a rejection sampling method, you balance the categories, resulting in a diverse set of 500,000+ samples.</p>
            </div>
        </details>
        
        <details>
            <summary>Step 3: Regenerate & Evaluate Replies</summary>
            <div class="step-content">
                <p>To get the highest quality data, you generate three different replies for each instruction: the <strong>original</strong> reply, a reply from a <strong>Large Language Model</strong> (Qwen2.5-7B-Instruct), and a reply from a <strong>Small Language Model</strong> (Qwen1.5-0.5B-Chat). You then evaluate them using perplexity, safety scores, and quality scores to pick the best one.</p>
            </div>
        </details>

        <details>
            <summary>Step 4: Final Dataset Construction</summary>
            <div class="step-content">
                <p>Using your evaluation scores, you create two distinct datasets:
                <ul>
                    <li>üóÉÔ∏è <strong>SFT Dataset (100,000 samples):</strong> Contains only the single best reply for each instruction, intended for Supervised Fine-Tuning.</li>
                    <li>‚öñÔ∏è <strong>KTO Dataset (150,000 samples):</strong> Contains both the best replies (positive feedback) and the worst replies (negative feedback) to form a preference dataset.</li>
                </ul>
                </p>
            </div>
        </details>
        
        <h2>Phase 2: The Experiments üß™</h2>
        <p>With your datasets ready, you'll train your baseline model, <strong>Qwen2-0.5B-Instruct</strong>, and evaluate its performance on standard benchmarks like HumanEval, ceval, and gsm8k.</p>

        <details>
            <summary>üî¨ Experiment 1: SFT Post-Training</summary>
            <div class="step-content">
                <p>First, you try continuous training on the baseline model using your 100k SFT dataset. After careful tuning, you find a small learning rate of 5e-8 works best.</p>
                <div class="results">
                    <h4>Results: Significant Improvement!</h4>
                    <p>This method clearly improves performance across all benchmarks.</p>
                    <table>
                        <tr><th>Evaluation Set</th><th>Baseline</th><th>SFT Model</th><th class="gain">Gain</th></tr>
                        <tr><td>Human Eval</td><td>17.1</td><td>27.44</td><td class="gain">+10.34</td></tr>
                        <tr><td>ceval</td><td>45.2</td><td>49.52</td><td class="gain">+4.32</td></tr>
                        <tr><td>gsm8k</td><td>40.1</td><td>42.00</td><td class="gain">+1.9</td></tr>
                    </table>
                </div>
            </div>
        </details>
        
        <details>
            <summary>üî¨ Experiment 2: Two-Stage SFT-KTO</summary>
            <div class="step-content">
                <p>Next, you combine methods in a two-stage experiment: first train with SFT, then continue training that new model with KTO.</p>
                <div class="results">
                    <h4>Results: The Best of Both Worlds!</h4>
                    <p>This two-stage approach yields even better results than either method alone.</p>
                    <table>
                        <tr><th>Evaluation Set</th><th>Baseline</th><th>SFT-KTO Model</th><th class="gain">Total Gain</th></tr>
                        <tr><td>Human Eval</td><td>17.1</td><td>28.05</td><td class="gain">+10.95</td></tr>
                        <tr><td>ceval</td><td>45.2</td><td>50.16</td><td class="gain">+4.96</td></tr>
                        <tr><td>gsm8k</td><td>40.1</td><td>42.5</td><td class="gain">+2.4</td></tr>
                    </table>
                </div>
            </div>
        </details>

        <h2>Phase 3: Final Touches üõ†Ô∏è</h2>
        <p>You've built powerful models. Can you do even better?</p>

        <details>
            <summary>üß© The Learning Rate Puzzle</summary>
            <div class="step-content">
                <p>During experiments, you noticed the learning rate is crucial for continuous post-training. The original pre-training rate of 1e-6 led to fast convergence but poor results. Only when you dropped the rate to <strong>5e-8</strong> did scores steadily improve, proving that a smaller, careful rate is better for targeted improvement.</p>
            </div>
        </details>

        <div class="final-conclusion">
            <h3>üèÜ Mission Accomplished! üèÜ</h3>
            <p>Congratulations, researcher! Through a carefully constructed data pipeline and a series of post-training experiments, you have successfully demonstrated that continuous post-training can significantly improve the performance of small language models. Your work on weight fusion and hyperparameter tuning further refines this process, paving the way for more powerful and efficient AI.</p>
        </div>
    </div>

</body>
</html>
