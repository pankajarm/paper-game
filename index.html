<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The SLM Optimization Challenge</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&display=swap" rel="stylesheet">
    <style>
        :root {
            --primary-color: #007bff;
            --secondary-color: #6c757d;
            --bg-color: #f8f9fa;
            --container-bg: #ffffff;
            --text-color: #212529;
            --border-color: #dee2e6;
            --success-color: #28a745;
            --warning-color: #ffc107;
        }

        body {
            font-family: 'Roboto', -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            background-color: var(--bg-color);
            margin: 0;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 20px auto;
            background-color: var(--container-bg);
            padding: 30px;
            border-radius: 12px;
            box-shadow: 0 8px 25px rgba(0,0,0,0.1);
        }

        h1, h2, h3 {
            color: #1c2d41;
            padding-bottom: 12px;
        }

        h1 {
            text-align: center;
            font-size: 2.8em;
            font-weight: 700;
            margin-bottom: 20px;
        }
        
        h2 {
            font-size: 2em;
            border-bottom: 2px solid var(--border-color);
            margin-top: 40px;
        }

        .mission-brief {
            background-color: #e6f2ff;
            border-left: 5px solid var(--primary-color);
            padding: 20px;
            margin: 25px 0;
            border-radius: 8px;
            font-size: 1.1em;
        }

        details {
            margin-bottom: 15px;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            overflow: hidden;
            transition: box-shadow 0.3s ease;
        }
        details:hover {
            box-shadow: 0 4px 15px rgba(0,0,0,0.08);
        }

        summary {
            font-weight: 500;
            font-size: 1.2em;
            padding: 18px;
            background-color: #fafafa;
            cursor: pointer;
            outline: none;
            list-style: none;
            position: relative;
        }
        
        summary::-webkit-details-marker {
            display: none;
        }

        summary:before {
            content: '‚û°Ô∏è';
            margin-right: 12px;
            display: inline-block;
            transition: transform 0.2s ease-in-out;
        }

        details[open] > summary:before {
            transform: rotate(90deg);
        }

        .step-content {
            padding: 20px;
            border-top: 1px solid var(--border-color);
            background-color: var(--container-bg);
        }

        .results {
            background-color: #f7fdf9;
            padding: 20px;
            margin-top: 20px;
            border: 1px solid #cce8d4;
            border-radius: 8px;
        }
        
        .results h4 {
            color: var(--success-color);
            font-size: 1.3em;
            margin-top: 0;
        }

        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin-top: 15px;
            border: 1px solid var(--border-color);
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
        }

        th, td {
            border: 1px solid var(--border-color);
            padding: 12px;
            text-align: left;
        }

        th {
            background-color: #f2f2f2;
            font-weight: 500;
        }
        
        tr:nth-child(even) {
            background-color: #f9f9f9;
        }

        .gain {
            color: var(--success-color);
            font-weight: bold;
        }

        .final-conclusion {
            background-color: #fffbeb;
            border-left: 5px solid var(--warning-color);
            padding: 25px;
            margin-top: 40px;
            border-radius: 8px;
        }
        
        .final-conclusion h3 {
            color: #b38600;
            margin-top: 0;
        }
    </style>
</head>
<body>

    <div class="container">
        <h1>üéÆ The SLM Optimization Challenge üéÆ</h1>
        <div class="mission-brief">
            <p><strong>Your Mission:</strong> You are an AI researcher tasked with enhancing a Small Language Model (SLM). The goal is to improve its performance without increasing its size, making it powerful yet efficient for devices like phones.</p>
            <p>This challenge is based on the paper: "A Post-Training Enhanced Optimization Approach for Small Language Models." Follow the steps, make the right choices, and see if you can achieve a breakthrough! üöÄ</p>
        </div>

        <h2>Phase 1: Building Your High-Quality Dataset üèóÔ∏è</h2>
        <p>A model is only as good as its data. Your first task is to construct a top-tier "alignment dataset" to train your SLM. The paper proposes a continuous post-training alignment data construction method based on the guidance of large models to optimize data diversity and accuracy.</p>

        <details>
            <summary>Step 1: Collect & Standardize Data</summary>
            <div class="step-content">
                [span_0](start_span)<p>You start by gathering data from various open-source datasets on the internet, covering topics like math, coding, and general instructions[span_0](end_span). [span_1](start_span)You collect a massive 5 million records[span_1](end_span)!</p>
                <ul>
                    [span_2](start_span)<li>Datasets include: BelleGroup, Coig, Camel, Dolly, and Alpaca_GPT4[span_2](end_span).</li>
                    [span_3](start_span)<li>You standardize the format of all this data and do a preliminary classification based on known types[span_3](end_span).</li>
                </ul>
            </div>
        </details>

        <details>
            <summary>Step 2: Deduplication & Diversity</summary>
            <div class="step-content">
                [span_4](start_span)<p>To avoid repetition, you use the <strong>simhash algorithm</strong> to find and remove duplicate prompts[span_4](end_span).</p>
                [span_5](start_span)[span_6](start_span)<p>To ensure data diversity, you use a larger model (Qwen2.5-7B-Instruct) to assign a secondary classification label to each instruction[span_5](end_span)[span_6](end_span). [span_7](start_span)Using a rejection sampling method, you balance the number of instructions in each category, resulting in a diverse set of 500,000+ samples[span_7](end_span).</p>
            </div>
        </details>
        
        <details>
            <summary>Step 3: Regenerate & Evaluate Replies</summary>
            <div class="step-content">
                <p>To get the highest quality data, you don't just use the existing replies. [span_8](start_span)[span_9](start_span)For each instruction, you generate three different replies[span_8](end_span)[span_9](end_span):</p>
                <ol>
                    [span_10](start_span)<li>The <strong>original</strong> reply from the dataset[span_10](end_span).</li>
                    [span_11](start_span)<li>A reply from a <strong>Large Language Model</strong> (Qwen2.5-7B-Instruct)[span_11](end_span).</li>
                    [span_12](start_span)<li>A reply from a <strong>Small Language Model</strong> (Qwen1.5-0.5B-Chat) [cite: 56-57].</li>
                </ol>
                <p>Now, how do you pick the best one? [cite_start]You use a three-pronged quality evaluation[span_12](end_span):</p>
                <ul>
                    <li><strong>Perplexity (PP):</strong> A measure of how "surprising" the text is to a model. [span_13](start_span)[span_14](start_span)A lower perplexity score indicates a better, more predictable reply[span_13](end_span)[span_14](end_span).</li>
                    [span_15](start_span)<li><strong>Safety Score:</strong> A large model rates the instruction and reply for safety on a 3-point scale[span_15](end_span).</li>
                    [span_16](start_span)<li><strong>Quality Score:</strong> A large model gives a comprehensive score for accuracy, relevance, and completeness, also on a 3-point scale[span_16](end_span).</li>
                </ul>
            </div>
        </details>

        <details>
            <summary>Step 4: Final Dataset Construction</summary>
            <div class="step-content">
                [span_17](start_span)<p>Using your evaluation scores, you create two distinct datasets for your experiments[span_17](end_span):</p>
                <ul>
                    <li>üóÉÔ∏è <strong>SFT Dataset (100,000 samples):</strong> For each instruction, you select the single best reply based on your quality metrics. [span_18](start_span)This is for Supervised Fine-Tuning[span_18](end_span).</li>
                    <li>‚öñÔ∏è <strong>KTO Dataset (150,000 samples):</strong> You take the best replies (positive feedback) and the worst replies (negative feedback) to form a preference dataset. [span_19](start_span)This is for Kahneman-Tversky Optimization[span_19](end_span).</li>
                </ul>
            </div>
        </details>
        
        <h2>Phase 2: The Experiments üß™</h2>
        <p>With your datasets ready, it's time to train! [span_20](start_span)Your baseline model is <strong>Qwen2-0.5B-Instruct</strong>[span_20](end_span). You'll use several post-training methods to see which one gives the best performance boost. [span_21](start_span)For your evaluation, you'll use standard industry benchmarks like mmlu, cmmlu, ceval, HumanEval, and gsm8k[span_21](end_span).</p>

        <details>
            <summary>üî¨ Experiment 1: SFT Continuous Post-Training</summary>
            <div class="step-content">
                [span_22](start_span)<p>You first try continuous training on your baseline model using your 100k SFT dataset[span_22](end_span). [span_23](start_span)After careful tuning, you find a small learning rate of 5e-8 works best[span_23](end_span).</p>
                <div class="results">
                    <h4>Results: Significant Improvement!</h4>
                    [span_24](start_span)<p>Continuous SFT post-training clearly improves the model's performance across all benchmarks[span_24](end_span).</p>
                    <table>
                        <tr>
                            <th>Evaluation Set</th>
                            <th>Baseline (Qwen2-0.5B)</th>
                            <th>Your SFT Model</th>
                            <th>Gain</th>
                        </tr>
                        <tr>
                            <td>Human Eval</td>
                            <td>17.1</td>
                            <td>27.44</td>
                            <td class="gain">+10.34</td>
                        </tr>
                        <tr>
                            <td>ceval</td>
                            <td>45.2</td>
                            <td>49.52</td>
                            <td class="gain">+4.32</td>
                        </tr>
                        <tr>
                            <td>gsm8k</td>
                            <td>40.1</td>
                            <td>42.00</td>
                            <td class="gain">+1.9</td>
                        </tr>
                    </table>
                </div>
            </div>
        </details>

        <details>
            <summary>üî¨ Experiment 2: KTO Continuous Post-Training</summary>
            <div class="step-content">
                [span_25](start_span)<p>Next, you test your KTO dataset, which uses both "good" and "bad" examples[span_25](end_span). [span_26](start_span)[span_27](start_span)KTO is designed to align models with human values by optimizing a loss function that considers loss aversion, and it doesn't require paired preference data[span_26](end_span)[span_27](end_span).</p>
                <div class="results">
                    <h4>Results: Also a Strong Success!</h4>
                    [span_28](start_span)<p>KTO training also significantly improves the model, with performance gains comparable to the SFT method[span_28](end_span).</p>
                     <table>
                        <tr>
                            <th>Evaluation Set</th>
                            <th>Baseline (Qwen2-0.5B)</th>
                            <th>Your KTO Model</th>
                            <th>Gain</th>
                        </tr>
                        <tr>
                            <td>HumanEval</td>
                            <td>17.1</td>
                            <td>27.44</td>
                            <td class="gain">+10.34</td>
                        </tr>
                         <tr>
                            <td>ceval</td>
                            <td>45.2</td>
                            <td>49.42</td>
                            <td class="gain">+4.22</td>
                        </tr>
                    </table>
                </div>
            </div>
        </details>
        
        <details>
            <summary>üî¨ Experiment 3: The SFT-KTO Two-Stage Combo</summary>
            <div class="step-content">
                <p>What if you combine the methods? [span_29](start_span)You decide to run a two-stage experiment: first, train with SFT, and then continue training that new model with KTO[span_29](end_span).</p>
                <div class="results">
                    <h4>Results: The Best of Both Worlds!</h4>
                    <p>Success! [span_30](start_span)The two-stage SFT-KTO approach yields even better results than either method alone, confirming the effectiveness of this combined strategy[span_30](end_span).</p>
                    <table>
                        <tr>
                            <th>Evaluation Set</th>
                            <th>Baseline</th>
                            <th>Your SFT-KTO Model</th>
                            <th class="gain">Total Gain</th>
                        </tr>
                         <tr>
                            <td>Human Eval</td>
                            <td>17.1</td>
                            <td>28.05</td>
                            <td class="gain">+10.95</td>
                        </tr>
                        <tr>
                            <td>ceval</td>
                            <td>45.2</td>
                            <td>50.16</td>
                            <td class="gain">+4.96</td>
                        </tr>
                        <tr>
                            <td>gsm8k</td>
                            <td>40.1</td>
                            <td>42.5</td>
                            <td class="gain">+2.4</td>
                        </tr>
                    </table>
                </div>
            </div>
        </details>

        <h2>Phase 3: The Final Touches üõ†Ô∏è</h2>
        <p>You have three powerful new models. Can you do even better?</p>

        <details>
            <summary>‚ú® Level Up: Weight Fusion</summary>
            <div class="step-content">
                [span_31](start_span)<p>You hypothesize that different training stages improve different aspects of the model[span_31](end_span). [span_32](start_span)So, you try fusing the weights of your three new models (SFT, KTO, and SFT-KTO) using a simple averaging strategy[span_32](end_span).</p>
                <div class="results">
                    <h4>Result: A Balanced Powerhouse!</h4>
                    <p>The weight fusion strategy works! [span_33](start_span)It enhances the overall performance and creates a more balanced model across all evaluation benchmarks[span_33](end_span).</p>
                </div>
            </div>
        </details>
        
        <details>
            <summary>üß© The Learning Rate Puzzle</summary>
            <div class="step-content">
                [span_34](start_span)<p>During your experiments, you noticed something crucial: the learning rate is incredibly important for continuous post-training[span_34](end_span).</p>
                <ul>
                    [span_35](start_span)<li>The original pre-training learning rate was 1e-6[span_35](end_span). [span_36](start_span)Using this rate led to fast convergence but poor overall results[span_36](end_span).</li>
                    [span_37](start_span)<li>Only when you dropped the learning rate by two orders of magnitude to <strong>5e-8</strong> did the scores begin to steadily improve, especially on complex tasks[span_37](end_span).</li>
                </ul>
                [span_38](start_span)<p>This shows that a higher learning rate might seem comprehensive but can harm performance in specific areas, while a smaller, careful rate is better for targeted improvement[span_38](end_span).</p>
            </div>
        </details>

        <div class="final-conclusion">
            <h3>üèÜ Mission Accomplished! üèÜ</h3>
            <p>Congratulations, researcher! [span_39](start_span)[span_40](start_span)Through a carefully constructed data pipeline and a series of post-training experiments (SFT, KTO, and two-stage SFT-KTO), you have successfully demonstrated that continuous post-training can significantly improve the performance of small language models[span_39](end_span)[span_40](end_span). [span_41](start_span)Your work on weight fusion and hyperparameter tuning further refines this process, paving the way for more powerful and efficient AI[span_41](end_span).</p>
        </div>

    </div>

</body>
</html>

