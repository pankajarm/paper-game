<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The SLM Optimization Challenge</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 20px auto;
            padding: 0 20px;
            background-color: #f4f7f9;
        }
        .container {
            background-color: #fff;
            padding: 25px;
            border-radius: 8px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        h1, h2, h3 {
            color: #1a2533;
            border-bottom: 2px solid #e1e8ed;
            padding-bottom: 10px;
        }
        h1 {
            text-align: center;
            font-size: 2.5em;
        }
        .mission-brief {
            background-color: #e1f5fe;
            border-left: 5px solid #0288d1;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        details {
            margin-bottom: 15px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        summary {
            font-weight: bold;
            padding: 15px;
            background-color: #f9f9f9;
            cursor: pointer;
            outline: none;
            list-style: '‚û°Ô∏è ';
        }
        details[open] > summary {
            background-color: #f0f0f0;
            list-style: '‚¨áÔ∏è ';
        }
        .step-content {
            padding: 15px;
            border-top: 1px solid #ddd;
        }
        .choice-box {
            display: flex;
            gap: 15px;
            justify-content: center;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .choice-btn {
            background-color: #4CAF50;
            color: white;
            padding: 14px 20px;
            margin: 8px 0;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            font-size: 16px;
        }
        .choice-btn:hover {
            background-color: #45a049;
        }
        .results {
            background-color: #eff;
            padding: 15px;
            margin-top: 15px;
            border: 1px dashed #99d;
            border-radius: 4px;
        }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin-top: 10px;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin-top: 15px;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        .gain {
            color: #2e7d32;
            font-weight: bold;
        }
        .final-conclusion {
            background-color: #fffde7;
            border-left: 5px solid #fbc02d;
            padding: 15px;
            margin-top: 25px;
            border-radius: 4px;
        }
    </style>
</head>
<body>

    <div class="container">
        <h1>üéÆ The SLM Optimization Challenge üéÆ</h1>
        <div class="mission-brief">
            <p><strong>Your Mission:</strong> You are an AI researcher tasked with enhancing a Small Language Model (SLM). [span_0](start_span)[span_1](start_span)The goal is to improve its performance without increasing its size, making it powerful yet efficient for devices like phones[span_0](end_span)[span_1](end_span).</p>
            [span_2](start_span)<p>This challenge is based on the paper: "A Post-Training Enhanced Optimization Approach for Small Language Models."[span_2](end_span) Follow the steps, make the right choices, and see if you can achieve a breakthrough! üöÄ</p>
        </div>

        <h2>Phase 1: Building Your High-Quality Dataset üèóÔ∏è</h2>
        <p>A model is only as good as its data. Your first task is to construct a top-tier "alignment dataset" to train your SLM. [span_3](start_span)[span_4](start_span)The paper proposes a continuous post-training alignment data construction method based on the guidance of large models to optimize data diversity and accuracy[span_3](end_span)[span_4](end_span).</p>

        <details>
            <summary>Step 1: Collect & Standardize Data</summary>
            <div class="step-content">
                [span_5](start_span)<p>You start by gathering data from various open-source datasets on the internet, covering topics like math, coding, and general instructions[span_5](end_span). [span_6](start_span)You collect a massive 5 million records[span_6](end_span)!</p>
                <ul>
                    [span_7](start_span)<li>Datasets include: BelleGroup, Coig, Camel, Dolly, and Alpaca_GPT4[span_7](end_span).</li>
                    [span_8](start_span)<li>You standardize the format of all this data and do a preliminary classification[span_8](end_span).</li>
                </ul>
            </div>
        </details>

        <details>
            <summary>Step 2: Deduplication & Diversity</summary>
            <div class="step-content">
                [span_9](start_span)<p>To avoid repetition, you use the <strong>simhash algorithm</strong> to find and remove duplicate prompts[span_9](end_span).</p>
                [span_10](start_span)[span_11](start_span)<p>To ensure data diversity, you use a larger model (Qwen2.5-7B-Instruct) to assign a secondary classification label to each instruction[span_10](end_span)[span_11](end_span). [span_12](start_span)Using a rejection sampling method, you balance the number of instructions in each category, resulting in a diverse set of 500,000+ samples[span_12](end_span).</p>
            </div>
        </details>
        
        <details>
            <summary>Step 3: Regenerate & Evaluate Replies</summary>
            <div class="step-content">
                <p>To get the highest quality data, you don't just use the existing replies. [span_13](start_span)[span_14](start_span)For each instruction, you generate three different replies[span_13](end_span)[span_14](end_span):</p>
                <ol>
                    <li>The <strong>original</strong> reply from the dataset.</li>
                    [span_15](start_span)<li>A reply from a <strong>Large Language Model</strong> (Qwen2.5-7B-Instruct)[span_15](end_span).</li>
                    [span_16](start_span)<li>A reply from a <strong>Small Language Model</strong> (Qwen1.5-0.5B-Chat)[span_16](end_span).</li>
                </ol>
                <p>Now, how do you pick the best one? [span_17](start_span)You use a three-pronged quality evaluation[span_17](end_span):</p>
                <ul>
                    <li><strong>Perplexity (PP):</strong> A measure of how "surprising" the text is to a model. [span_18](start_span)[span_19](start_span)A lower perplexity score indicates a better, more predictable reply[span_18](end_span)[span_19](end_span).</li>
                    [span_20](start_span)<li><strong>Safety Score:</strong> A large model rates the instruction and reply for safety on a 3-point scale[span_20](end_span).</li>
                    [span_21](start_span)<li><strong>Quality Score:</strong> A large model gives a comprehensive score for accuracy, relevance, and completeness, also on a 3-point scale[span_21](end_span).</li>
                </ul>
            </div>
        </details>

        <details>
            <summary>Step 4: Final Dataset Construction</summary>
            <div class="step-content">
                <p>Using your evaluation scores, you create two distinct datasets for your experiments:</p>
                <ul>
                    <li>üóÉÔ∏è <strong>SFT Dataset (100,000 samples):</strong> For each instruction, you select the single best reply based on your quality metrics. [span_22](start_span)This is for Supervised Fine-Tuning[span_22](end_span).</li>
                    <li>‚öñÔ∏è <strong>KTO Dataset (150,000 samples):</strong> You take the best replies (positive feedback) and the worst replies (negative feedback) to form a preference dataset. [span_23](start_span)This is for Kahneman-Tversky Optimization[span_23](end_span).</li>
                </ul>
            </div>
        </details>
        
        <h2>Phase 2: The Experiments üß™</h2>
        <p>With your datasets ready, it's time to train! [span_24](start_span)Your baseline model is <strong>Qwen2-0.5B-Instruct</strong>[span_24](end_span). You'll use several post-training methods to see which one gives the best performance boost.</p>
        [span_25](start_span)<p>For your evaluation, you'll use standard industry benchmarks like mmlu, cmmlu, ceval, HumanEval, and gsm8k[span_25](end_span).</p>

        <details>
            <summary>üî¨ Experiment 1: SFT Continuous Post-Training</summary>
            <div class="step-content">
                [span_26](start_span)<p>You first try continuous training on your baseline model using your 100k SFT dataset[span_26](end_span). [span_27](start_span)After careful tuning, you find a small learning rate of 5e-8 works best[span_27](end_span).</p>
                <div class="results">
                    <h4>Results: Significant Improvement!</h4>
                    [span_28](start_span)<p>Continuous SFT post-training clearly improves the model's performance across all benchmarks[span_28](end_span).</p>
                    <table>
                        <tr>
                            <th>Evaluation Set</th>
                            <th>Baseline (Qwen2-0.5B)</th>
                            <th>Your SFT Model</th>
                            <th>Gain</th>
                        </tr>
                        <tr>
                            <td>mmlu</td>
                            <td>37.9</td>
                            <td>39.07</td>
                            <td class="gain">+1.17</td>
                        </tr>
                        <tr>
                            <td>cmmlu</td>
                            <td>45.48</td>
                            <td>45.80</td>
                            <td class="gain">+0.32</td>
                        </tr>
                        <tr>
                            <td>ceval</td>
                            <td>45.2</td>
                            <td>49.52</td>
                            <td class="gain">+4.32</td>
                        </tr>
                        <tr>
                            <td>Human Eval</td>
                            <td>17.1</td>
                            <td>27.44</td>
                            <td class="gain">+10.34</td>
                        </tr>
                        <tr>
                            <td>gsm8k</td>
                            <td>40.1</td>
                            <td>42.00</td>
                            <td class="gain">+1.9</td>
                        </tr>
                    </table>
                </div>
            </div>
        </details>

        <details>
            <summary>üî¨ Experiment 2: KTO Continuous Post-Training</summary>
            <div class="step-content">
                [span_29](start_span)<p>Next, you test your KTO dataset, which uses both "good" and "bad" examples[span_29](end_span). [span_30](start_span)KTO is designed to align models with human values by optimizing a loss function that considers loss aversion, and it doesn't require paired preference data[span_30](end_span).</p>
                <div class="results">
                    <h4>Results: Also a Strong Success!</h4>
                    [span_31](start_span)<p>KTO training also significantly improves the model, with performance gains comparable to the SFT method[span_31](end_span).</p>
                    <table>
                        <tr>
                            <th>Evaluation Set</th>
                            <th>Baseline (Qwen2-0.5B)</th>
                            <th>Your KTO Model</th>
                            <th>Gain</th>
                        </tr>
                        <tr>
                            <td>HumanEval</td>
                            <td>17.1</td>
                            <td>27.44</td>
                            <td class="gain">+10.34</td>
                        </tr>
                         <tr>
                            <td>ceval</td>
                            <td>45.2</td>
                            <td>49.42</td>
                            <td class="gain">+4.22</td>
                        </tr>
                    </table>
                </div>
            </div>
        </details>
        
        <details>
            <summary>üî¨ Experiment 3: The SFT-KTO Two-Stage Combo</summary>
            <div class="step-content">
                <p>What if you combine the methods? [span_32](start_span)You decide to run a two-stage experiment: first, train with SFT, and then continue training that new model with KTO[span_32](end_span).</p>
                <div class="results">
                    <h4>Results: The Best of Both Worlds!</h4>
                    <p>Success! [span_33](start_span)The two-stage SFT-KTO approach yields even better results than either method alone, confirming the effectiveness of this combined strategy[span_33](end_span).</p>
                    <table>
                        <tr>
                            <th>Evaluation Set</th>
                            <th>Baseline</th>
                            <th>Your SFT-KTO Model</th>
                            <th class="gain">Total Gain</th>
                        </tr>
                        <tr>
                            <td>mmlu</td>
                            <td>37.9</td>
                            <td>39.32</td>
                            <td class="gain">+1.42</td>
                        </tr>
                        <tr>
                            <td>cmmlu</td>
                            <td>45.48</td>
                            <td>47.4</td>
                            <td class="gain">+1.92</td>
                        </tr>
                        <tr>
                            <td>ceval</td>
                            <td>45.2</td>
                            <td>50.16</td>
                            <td class="gain">+4.96</td>
                        </tr>
                        <tr>
                            <td>Human Eval</td>
                            <td>17.1</td>
                            <td>28.05</td>
                            <td class="gain">+10.95</td>
                        </tr>
                        <tr>
                            <td>gsm8k</td>
                            <td>40.1</td>
                            <td>42.5</td>
                            <td class="gain">+2.4</td>
                        </tr>
                    </table>
                </div>
            </div>
        </details>

        <h2>Phase 3: The Final Touches üõ†Ô∏è</h2>
        <p>You have three powerful new models. Can you do even better?</p>

        <details>
            <summary>‚ú® Level Up: Weight Fusion</summary>
            <div class="step-content">
                [span_34](start_span)<p>You hypothesize that different training stages improve different aspects of the model[span_34](end_span). [span_35](start_span)So, you try fusing the weights of your three new models (SFT, KTO, and SFT-KTO) using a simple averaging strategy[span_35](end_span).</p>
                <div class="results">
                    <h4>Result: A Balanced Powerhouse!</h4>
                    <p>The weight fusion strategy works! [span_36](start_span)It enhances the overall performance and creates a more balanced model across all evaluation benchmarks[span_36](end_span).</p>
                    <img src="https://i.imgur.com/k6rD3F7.png" alt="Bar chart showing the impact of weight integration on benchmark indicators. The 'Merge' model shows strong, balanced performance across mmlu, cmmlu, ceval, HumanEval, and gsm8k.">
                </div>
            </div>
        </details>
        
        <details>
            <summary>üß© The Learning Rate Puzzle</summary>
            <div class="step-content">
                [span_37](start_span)<p>During your experiments, you noticed something crucial: the learning rate is incredibly important for continuous post-training[span_37](end_span).</p>
                <ul>
                    [span_38](start_span)<li>The original pre-training learning rate was 1e-6[span_38](end_span). [span_39](start_span)Using this rate led to fast convergence but poor overall results[span_39](end_span).</li>
                    [span_40](start_span)<li>Only when you dropped the learning rate by two orders of magnitude to <strong>5e-8</strong> did the scores begin to steadily improve, especially on complex tasks[span_40](end_span).</li>
                </ul>
                [span_41](start_span)<p>This shows that a higher learning rate might seem comprehensive but can harm performance in specific areas, while a smaller, careful rate is better for targeted improvement[span_41](end_span).</p>
                <img src="https://i.imgur.com/vHqjHn3.png" alt="Chart showing that a learning rate of 5.00E-08 provides better results on benchmarks like HumanEval and gsm8k compared to the baseline and a higher learning rate of 1.00E-06.">
            </div>
        </details>

        <div class="final-conclusion">
            <h3>üèÜ Mission Accomplished! üèÜ</h3>
            <p>Congratulations, researcher! [span_42](start_span)[span_43](start_span)Through a carefully constructed data pipeline and a series of post-training experiments (SFT, KTO, and two-stage SFT-KTO), you have successfully demonstrated that continuous post-training can significantly improve the performance of small language models[span_42](end_span)[span_43](end_span). Your work on weight fusion and hyperparameter tuning further refines this process, paving the way for more powerful and efficient AI.</p>
        </div>

    </div>

</body>
</html>
